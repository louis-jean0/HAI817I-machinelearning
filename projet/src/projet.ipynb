{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapes\n",
    "\n",
    "1. Fusionner les jeux de données de test et de train\n",
    "2. Appliquer des pré-traitements (élimination des stop words, passage en minuscules/majuscules, lemmatisation, racinisation (stemming), tokenisation, vectorisation, NER, POS tagging, ...) et sauvegarder chacun des jeux de données traités pour gagner du temps (comme expliqué par M. Poncelet en TD)\n",
    "3. Ingénierie des données : choisir une méthode de représentation du texte (par exemple bag of words ou tf-idf) de manière à ce qu'il soit compréhensible et utilisable pour un modèle de classification, sortir des features. Potentiellement : modélisation par sujets (topic modeling), reconnaissance d'entités (entity recognition).\n",
    "4. Choisir un classifieur adapté. Nous nous intéresserons à trois tâches de classification :\n",
    "\n",
    "    - {VRAI} vs. {FAUX} (deux classes)\n",
    "    - {VRAI ou FAUX} vs. {AUTRE} (deux classes)\n",
    "    - {VRAI} vs. {FAUX} vs. {MIXTE} vs. {AUTRE} (quatre classes)\n",
    "\n",
    "    Appliquer de l'upsampling ou du downsampling pour équilibrer les données.\n",
    "    \n",
    "    Modèles de classification envisagés : arbres de décision, SVMs, Naïve Bayes, les K-NN, les random forests, etc.\n",
    "5. Sélection de variable (feature selection): pour chacune des trois tâches de classification, en plus des modèles de classification, préparer une liste de features discriminantes en ordre décroissant. Pour cela, s'appuyer sur des méthodes de sélection de variables (ou de features). Tirer les conclusions.\n",
    "6. Analyse des erreurs, validation et comparaisons des modèles : comparer empiriquement les différents choix quevous avez pu faire dans la partie sélection des features, des prétraitements, des modèles\n",
    "utilisés, de l'échantillonnage, etc. par rapport à leur impact sur la qualité de la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des dépendances nécessaires au projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Louis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Louis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Louis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Louis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première étape : fusionner les jeux de données de test et de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes train : 1264\n",
      "Nb lignes test : 612\n",
      "Nb lignes combiné : 1876\n"
     ]
    }
   ],
   "source": [
    "train_dataframe = pd.read_csv(\"../data/HAI817_Projet_train.csv\")\n",
    "test_dataframe = pd.read_csv(\"../data/HAI817_Projet_test.csv\")\n",
    "print(\"Nb lignes train : \" + str(len(train_dataframe)))\n",
    "print(\"Nb lignes test : \" + str(len(test_dataframe)))\n",
    "dataframe = pd.concat([train_dataframe,test_dataframe])\n",
    "print(\"Nb lignes combiné : \" + str(len(dataframe)))\n",
    "dataframe.to_csv(\"../data/combined_dataframe.csv\")\n",
    "dataframe = pd.read_csv(\"../data/combined_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième étape : appliquer des prétraitements sur le jeu de données\n",
    "\n",
    "#### Ici, on écrit les fonctions de prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre en minuscule\n",
    "def to_lower_case(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.lower()\n",
    "\n",
    "# Mettre en majuscule\n",
    "def to_upper_case(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.upper()\n",
    "\n",
    "# Générer un wordcloud\n",
    "def generate_wordcloud(data):\n",
    "    text = ' '.join(data)\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                          background_color ='white', \n",
    "                          stopwords = set(stopwords.words('english')), \n",
    "                          min_font_size = 10).generate(text)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "\n",
    "# Suppression des caractères spéciaux, des chiffres, des stopwords et lemmatisation\n",
    "def basic_preprocessing(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Supprimer les caractères non-alphabétiques et les chiffres\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text, re.I|re.A)\n",
    "    # Suppression des stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "    return ' '.join(lemma_words)\n",
    "\n",
    "# Stemming\n",
    "def stemming(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    stemmer = PorterStemmer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# POS tagging\n",
    "def pos_tagging(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(word_tokens)\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On créé les nouveaux datasets prétraités\n",
    "\n",
    "**Les nouveaux datasets sont créés à partir du dataset combiné obtenu à l'étape 1**\n",
    "\n",
    "À chaque fois, on ne traite que le texte et le titre des articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset en minuscules\n",
    "lower_case_dataframe = dataframe.copy()\n",
    "lower_case_dataframe[\"text\"] = lower_case_dataframe[\"text\"].apply(to_lower_case)\n",
    "lower_case_dataframe[\"title\"] = lower_case_dataframe[\"title\"].apply(to_lower_case)\n",
    "lower_case_dataframe.to_csv(\"../data/lower.csv\", index=False)\n",
    "\n",
    "# Dataset en majuscules\n",
    "upper_case_dataframe = dataframe.copy()\n",
    "upper_case_dataframe[\"text\"] = upper_case_dataframe[\"text\"].apply(to_upper_case)\n",
    "upper_case_dataframe[\"title\"] = upper_case_dataframe[\"title\"].apply(to_upper_case)\n",
    "upper_case_dataframe.to_csv(\"../data/upper.csv\", index=False)\n",
    "\n",
    "# Dataset avec suppression des caractères spéciaux, des chiffres, des stopwords et lemmatisation\n",
    "basic_preprocessed_dataframe = dataframe.copy()\n",
    "basic_preprocessed_dataframe[\"text\"] = basic_preprocessed_dataframe[\"text\"].apply(basic_preprocessing)\n",
    "basic_preprocessed_dataframe[\"title\"] = basic_preprocessed_dataframe[\"title\"].apply(basic_preprocessing)\n",
    "basic_preprocessed_dataframe.to_csv(\"../data/basic_preprocessing.csv\", index=False)\n",
    "\n",
    "# Dataset avec stemming\n",
    "stemmed_dataframe = dataframe.copy()\n",
    "stemmed_dataframe[\"text\"] = stemmed_dataframe[\"text\"].apply(stemming)\n",
    "stemmed_dataframe[\"title\"] = stemmed_dataframe[\"title\"].apply(stemming)\n",
    "stemmed_dataframe.to_csv(\"../data/stemming.csv\", index=False)\n",
    "\n",
    "# Dataset avec POS tagging\n",
    "pos_tagged_dataframe = dataframe.copy()\n",
    "pos_tagged_dataframe[\"text\"] = pos_tagged_dataframe[\"text\"].apply(pos_tagging)\n",
    "pos_tagged_dataframe[\"title\"] = pos_tagged_dataframe[\"title\"].apply(pos_tagging)\n",
    "pos_tagged_dataframe.to_csv(\"../data/pos_tagging.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troisième étape : ingénierie des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (4, 1)\t1\n",
      "  (4, 3)\t1\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(basic_preprocessed_dataframe)\n",
    "print(X_bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
